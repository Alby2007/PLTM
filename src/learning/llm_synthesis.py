"""
LLM-Powered Cross-Domain Synthesis

Puts Claude's reasoning into the discovery loop:
- Synthesize insights across domain knowledge subgraphs
- Generate novel hypotheses from accumulated knowledge
- Evaluate hypotheses against evidence
- Find structural analogies between domains

Token-efficient: compressed atom summaries, structured JSON output,
tight max_tokens budgets, 24h caching via HYPOTHESIS atoms.
"""

import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field

from src.storage.sqlite_store import SQLiteGraphStore
from src.core.models import MemoryAtom, AtomType, Provenance, GraphType
from loguru import logger


@dataclass
class SynthesisInsight:
    """A novel insight from LLM cross-domain synthesis"""
    insight: str
    domains: List[str]
    confidence: float
    evidence: List[str]
    novelty: float
    discovered_at: datetime = field(default_factory=datetime.now)


@dataclass
class Hypothesis:
    """A testable hypothesis generated by LLM"""
    hypothesis_id: str
    statement: str
    domains: List[str]
    testable_prediction: str
    confidence: float
    source_evidence: List[str]


@dataclass
class HypothesisEvaluation:
    """LLM evaluation of a hypothesis against evidence"""
    supported: bool
    confidence: float
    reasoning: str
    evidence_strength: float


@dataclass
class Analogy:
    """A structural analogy between domains"""
    source_concept: str
    source_domain: str
    target_concept: str
    target_domain: str
    mapping: str
    confidence: float


class LLMSynthesizer:
    """
    Uses Claude to reason over knowledge subgraphs and discover
    non-obvious connections, analogies, and hypotheses.
    
    Token-efficient design:
    - Atoms compressed to one-liners before prompting
    - Structured JSON output (no prose)
    - Tight max_tokens per call type
    - Results cached as HYPOTHESIS atoms (24h dedup)
    """
    
    def __init__(self, store: SQLiteGraphStore, model: str = "claude-sonnet-4-20250514"):
        self.store = store
        self.model = model
        self.api_key = os.getenv("ANTHROPIC_API_KEY", "")
        self.enabled = bool(self.api_key)
        self._synthesis_cache: Dict[str, datetime] = {}
        
        if not self.enabled:
            logger.warning("ANTHROPIC_API_KEY not set - LLM synthesis disabled")
        else:
            logger.info("LLMSynthesizer initialized")
    
    # ========== CORE METHODS ==========
    
    async def synthesize_cross_domain(
        self,
        domain_a: str,
        domain_b: str,
        atoms_a: List[MemoryAtom],
        atoms_b: List[MemoryAtom],
    ) -> List[SynthesisInsight]:
        """
        Ask Claude to find non-obvious connections between two domain subgraphs.
        
        Token budget: ≤2K input, ≤1K output
        """
        if not self.enabled:
            return []
        
        # Check 24h cache
        cache_key = f"{sorted([domain_a, domain_b])}"
        if cache_key in self._synthesis_cache:
            if datetime.now() - self._synthesis_cache[cache_key] < timedelta(hours=24):
                logger.debug(f"Synthesis cache hit for {domain_a}<->{domain_b}")
                return []
        
        summary_a = self._compress_atoms(atoms_a, max_atoms=30)
        summary_b = self._compress_atoms(atoms_b, max_atoms=30)
        
        prompt = f"""You are a cross-domain insight engine. Find non-obvious connections between these two knowledge domains.

DOMAIN A ({domain_a}):
{summary_a}

DOMAIN B ({domain_b}):
{summary_b}

Return a JSON array of insights. Each insight must be a genuinely novel connection, not obvious.
Format: [{{"insight":"...","domains":["{domain_a}","{domain_b}"],"confidence":0.0-1.0,"evidence":["fact1","fact2"],"novelty":0.0-1.0}}]

Rules:
- Only include insights that connect BOTH domains
- Confidence reflects how well-supported the connection is
- Novelty reflects how surprising/non-obvious it is
- Evidence lists the specific facts that support the insight
- Maximum 5 insights
- Be specific, not generic"""

        response = await self._call_llm(prompt, max_tokens=1024)
        insights = self._parse_insights(response, domain_a, domain_b)
        
        # Cache and store
        self._synthesis_cache[cache_key] = datetime.now()
        await self._store_insights(insights)
        
        return insights
    
    async def generate_novel_hypotheses(
        self,
        knowledge_atoms: List[MemoryAtom],
        open_questions: Optional[List[str]] = None,
        domains: Optional[List[str]] = None,
    ) -> List[Hypothesis]:
        """
        Generate testable hypotheses from accumulated knowledge.
        
        Token budget: ≤1.5K input, ≤512 output
        """
        if not self.enabled:
            return []
        
        summary = self._compress_atoms(knowledge_atoms, max_atoms=40)
        
        questions_block = ""
        if open_questions:
            q_list = "\n".join(f"- {q}" for q in open_questions[:5])
            questions_block = f"\nOPEN QUESTIONS:\n{q_list}\n"
        
        domain_str = ", ".join(domains) if domains else "general"
        
        prompt = f"""You are a hypothesis generation engine. Given this knowledge base, generate novel testable hypotheses.

KNOWLEDGE ({domain_str}):
{summary}
{questions_block}
Return a JSON array of hypotheses.
Format: [{{"statement":"...","domains":[...],"testable_prediction":"...","confidence":0.0-1.0,"source_evidence":["fact1","fact2"]}}]

Rules:
- Each hypothesis must be TESTABLE (has a falsifiable prediction)
- Focus on non-obvious implications of the knowledge
- If open questions provided, try to address them
- Maximum 5 hypotheses
- Be specific and precise"""

        response = await self._call_llm(prompt, max_tokens=512)
        return self._parse_hypotheses(response, domains or [])
    
    async def evaluate_hypothesis(
        self,
        hypothesis: str,
        evidence_atoms: List[MemoryAtom],
    ) -> HypothesisEvaluation:
        """
        Evaluate a hypothesis against accumulated evidence.
        
        Token budget: ≤1.5K input, ≤256 output
        """
        if not self.enabled:
            return HypothesisEvaluation(
                supported=False, confidence=0.0,
                reasoning="LLM disabled", evidence_strength=0.0
            )
        
        evidence = self._compress_atoms(evidence_atoms, max_atoms=25)
        
        prompt = f"""Evaluate this hypothesis against the evidence.

HYPOTHESIS: {hypothesis[:200]}

EVIDENCE:
{evidence}

Return JSON: {{"supported":true/false,"confidence":0.0-1.0,"reasoning":"brief explanation","evidence_strength":0.0-1.0}}

Rules:
- supported: does evidence support the hypothesis?
- confidence: how confident in the evaluation
- reasoning: 1-2 sentences max
- evidence_strength: how strong is the evidence (0=none, 1=conclusive)"""

        response = await self._call_llm(prompt, max_tokens=256)
        return self._parse_evaluation(response)
    
    async def find_analogies(
        self,
        concept: str,
        source_domain: str,
        target_domain: str,
        source_atoms: List[MemoryAtom],
        target_atoms: List[MemoryAtom],
    ) -> List[Analogy]:
        """
        Find structural analogies between domains for a concept.
        
        Token budget: ≤1.5K input, ≤512 output
        """
        if not self.enabled:
            return []
        
        src_summary = self._compress_atoms(source_atoms, max_atoms=20)
        tgt_summary = self._compress_atoms(target_atoms, max_atoms=20)
        
        prompt = f"""Find structural analogies for "{concept}" between these domains.

SOURCE DOMAIN ({source_domain}):
{src_summary}

TARGET DOMAIN ({target_domain}):
{tgt_summary}

Return JSON array of analogies.
Format: [{{"source_concept":"...","source_domain":"{source_domain}","target_concept":"...","target_domain":"{target_domain}","mapping":"X in {source_domain} is like Y in {target_domain} because...","confidence":0.0-1.0}}]

Rules:
- Focus on STRUCTURAL similarities (same role/function), not surface similarities
- Maximum 5 analogies
- Be specific about WHY the analogy holds"""

        response = await self._call_llm(prompt, max_tokens=512)
        return self._parse_analogies(response)
    
    # ========== COMPRESSION ==========
    
    def _compress_atoms(self, atoms: List[MemoryAtom], max_atoms: int = 30) -> str:
        """
        Compress atoms into dense one-liner summaries for LLM prompts.
        
        Sorted by confidence desc, deduped, truncated.
        ~10x token reduction vs raw serialization.
        """
        if not atoms:
            return "(empty)"
        
        # Deduplicate by (subject, predicate, object)
        seen = set()
        unique = []
        for a in atoms:
            key = (a.subject.lower(), a.predicate.lower(), a.object.lower()[:50])
            if key not in seen:
                seen.add(key)
                unique.append(a)
        
        # Sort by confidence descending
        unique.sort(key=lambda a: a.confidence, reverse=True)
        
        # Take top N and format as one-liners
        lines = []
        for a in unique[:max_atoms]:
            obj_truncated = a.object[:80] if len(a.object) > 80 else a.object
            lines.append(f"- {a.subject} {a.predicate} {obj_truncated} (c:{a.confidence:.1f})")
        
        return "\n".join(lines)
    
    # ========== LLM CALL ==========
    
    async def _call_llm(self, prompt: str, max_tokens: int = 512) -> str:
        """Call Claude API with token-efficient settings"""
        try:
            import anthropic
            
            client = anthropic.Anthropic(api_key=self.api_key)
            
            message = client.messages.create(
                model=self.model,
                max_tokens=max_tokens,
                temperature=0.3,
                messages=[{"role": "user", "content": prompt}]
            )
            
            return message.content[0].text
        except Exception as e:
            logger.error(f"LLM synthesis call failed: {e}")
            return "[]"
    
    # ========== PARSERS ==========
    
    def _parse_insights(
        self, response: str, domain_a: str, domain_b: str
    ) -> List[SynthesisInsight]:
        """Parse LLM JSON response into SynthesisInsight objects"""
        try:
            data = self._extract_json_array(response)
            insights = []
            for item in data[:5]:
                insights.append(SynthesisInsight(
                    insight=str(item.get("insight", ""))[:300],
                    domains=item.get("domains", [domain_a, domain_b]),
                    confidence=float(item.get("confidence", 0.5)),
                    evidence=[str(e)[:100] for e in item.get("evidence", [])[:5]],
                    novelty=float(item.get("novelty", 0.5)),
                ))
            return insights
        except Exception as e:
            logger.warning(f"Failed to parse synthesis insights: {e}")
            return []
    
    def _parse_hypotheses(
        self, response: str, domains: List[str]
    ) -> List[Hypothesis]:
        """Parse LLM JSON response into Hypothesis objects"""
        try:
            data = self._extract_json_array(response)
            hypotheses = []
            for i, item in enumerate(data[:5]):
                hypotheses.append(Hypothesis(
                    hypothesis_id=f"hyp_llm_{datetime.now().strftime('%Y%m%d%H%M')}_{i}",
                    statement=str(item.get("statement", ""))[:300],
                    domains=item.get("domains", domains),
                    testable_prediction=str(item.get("testable_prediction", ""))[:200],
                    confidence=float(item.get("confidence", 0.5)),
                    source_evidence=[str(e)[:100] for e in item.get("source_evidence", [])[:5]],
                ))
            return hypotheses
        except Exception as e:
            logger.warning(f"Failed to parse hypotheses: {e}")
            return []
    
    def _parse_evaluation(self, response: str) -> HypothesisEvaluation:
        """Parse LLM JSON response into HypothesisEvaluation"""
        try:
            data = self._extract_json_object(response)
            return HypothesisEvaluation(
                supported=bool(data.get("supported", False)),
                confidence=float(data.get("confidence", 0.5)),
                reasoning=str(data.get("reasoning", ""))[:200],
                evidence_strength=float(data.get("evidence_strength", 0.0)),
            )
        except Exception as e:
            logger.warning(f"Failed to parse evaluation: {e}")
            return HypothesisEvaluation(
                supported=False, confidence=0.0,
                reasoning=f"Parse error: {e}", evidence_strength=0.0
            )
    
    def _parse_analogies(self, response: str) -> List[Analogy]:
        """Parse LLM JSON response into Analogy objects"""
        try:
            data = self._extract_json_array(response)
            analogies = []
            for item in data[:5]:
                analogies.append(Analogy(
                    source_concept=str(item.get("source_concept", ""))[:100],
                    source_domain=str(item.get("source_domain", ""))[:50],
                    target_concept=str(item.get("target_concept", ""))[:100],
                    target_domain=str(item.get("target_domain", ""))[:50],
                    mapping=str(item.get("mapping", ""))[:200],
                    confidence=float(item.get("confidence", 0.5)),
                ))
            return analogies
        except Exception as e:
            logger.warning(f"Failed to parse analogies: {e}")
            return []
    
    def _extract_json_array(self, text: str) -> List[Dict]:
        """Extract JSON array from LLM response (handles markdown fences)"""
        text = text.strip()
        # Strip markdown code fences
        if text.startswith("```"):
            lines = text.split("\n")
            text = "\n".join(lines[1:-1] if lines[-1].strip() == "```" else lines[1:])
        # Find array
        start = text.find("[")
        end = text.rfind("]")
        if start >= 0 and end > start:
            return json.loads(text[start:end + 1])
        return []
    
    def _extract_json_object(self, text: str) -> Dict:
        """Extract JSON object from LLM response"""
        text = text.strip()
        if text.startswith("```"):
            lines = text.split("\n")
            text = "\n".join(lines[1:-1] if lines[-1].strip() == "```" else lines[1:])
        start = text.find("{")
        end = text.rfind("}")
        if start >= 0 and end > start:
            return json.loads(text[start:end + 1])
        return {}
    
    # ========== STORAGE ==========
    
    async def _store_insights(self, insights: List[SynthesisInsight]) -> None:
        """Store LLM-generated insights as HYPOTHESIS atoms"""
        for insight in insights[:10]:
            atom = MemoryAtom(
                atom_type=AtomType.HYPOTHESIS,
                subject=f"llm_insight:{','.join(insight.domains[:3])}",
                predicate="cross_domain_insight",
                object=insight.insight[:300],
                confidence=insight.confidence,
                strength=insight.novelty,
                provenance=Provenance.INFERRED,
                source_user="llm_synthesis",
                contexts=["llm_insight", "cross_domain"] + insight.domains[:5],
                graph=GraphType.SUBSTANTIATED,
            )
            await self.store.add_atom(atom)
        
        if insights:
            logger.info(f"Stored {len(insights)} LLM synthesis insights")
    
    async def store_hypotheses(self, hypotheses: List[Hypothesis]) -> None:
        """Store LLM-generated hypotheses as atoms"""
        for hyp in hypotheses[:10]:
            atom = MemoryAtom(
                atom_type=AtomType.HYPOTHESIS,
                subject=f"llm_hypothesis:{hyp.hypothesis_id}",
                predicate="hypothesizes",
                object=hyp.statement[:300],
                confidence=hyp.confidence,
                strength=hyp.confidence,
                provenance=Provenance.INFERRED,
                source_user="llm_synthesis",
                contexts=["llm_hypothesis"] + hyp.domains[:5],
                graph=GraphType.UNSUBSTANTIATED,
            )
            await self.store.add_atom(atom)
        
        if hypotheses:
            logger.info(f"Stored {len(hypotheses)} LLM hypotheses")
    
    # ========== COMPACT RESULTS ==========
    
    def insights_to_compact(self, insights: List[SynthesisInsight]) -> Dict[str, Any]:
        """Token-efficient serialization for MCP responses"""
        return {
            "ok": True,
            "n": len(insights),
            "insights": [
                {
                    "i": ins.insight[:100],
                    "d": ins.domains[:3],
                    "c": round(ins.confidence, 2),
                    "nov": round(ins.novelty, 2),
                }
                for ins in insights[:5]
            ],
        }
    
    def hypotheses_to_compact(self, hypotheses: List[Hypothesis]) -> Dict[str, Any]:
        """Token-efficient serialization for MCP responses"""
        return {
            "ok": True,
            "n": len(hypotheses),
            "hypotheses": [
                {
                    "id": h.hypothesis_id,
                    "s": h.statement[:100],
                    "d": h.domains[:3],
                    "pred": h.testable_prediction[:80],
                    "c": round(h.confidence, 2),
                }
                for h in hypotheses[:5]
            ],
        }
    
    def evaluation_to_compact(self, ev: HypothesisEvaluation) -> Dict[str, Any]:
        """Token-efficient serialization for MCP responses"""
        return {
            "ok": True,
            "supported": ev.supported,
            "c": round(ev.confidence, 2),
            "reason": ev.reasoning[:100],
            "strength": round(ev.evidence_strength, 2),
        }
    
    def analogies_to_compact(self, analogies: List[Analogy]) -> Dict[str, Any]:
        """Token-efficient serialization for MCP responses"""
        return {
            "ok": True,
            "n": len(analogies),
            "analogies": [
                {
                    "src": f"{a.source_concept} ({a.source_domain})",
                    "tgt": f"{a.target_concept} ({a.target_domain})",
                    "map": a.mapping[:100],
                    "c": round(a.confidence, 2),
                }
                for a in analogies[:5]
            ],
        }
